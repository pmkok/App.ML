{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192e6f52-faed-4116-9816-240b09ec95f6",
   "metadata": {},
   "source": [
    "# Road Sign Recognition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad86eb8-90bb-48d3-8721-eb35450ec949",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Problem](#Problem)\n",
    "3. [Methods](#Methods)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633d361-737d-4606-9f31-04a1d76c2896",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "\n",
    "Artificial Intelligence (AI) has emerged as a powerful tool for solving complex problems and transforming various industries. Its ability to process large volumes of data, learn from patterns, and make intelligent decisions has made it a valuable asset in problem-solving scenarios. AI offers several advantages that enable the resolution of problems in a simpler and more efficient manner. One of the key strengths of AI is its capacity to handle and analyze massive amounts of data quickly and accurately. Traditional problem-solving approaches often struggle with large datasets or complex patterns that are difficult for humans to comprehend. AI algorithms, on the other hand, can effortlessly process and extract valuable insights from vast data sets, enabling the identification of hidden patterns or correlations that might not be evident to human observers. This capability allows AI to tackle complex problems with ease.\n",
    "\n",
    "## Problem <a name=\"Problem\"></a>\n",
    "\n",
    "\n",
    "The accurate detection and interpretation of road signs play a vital role in ensuring road safety and efficient traffic management. However, traditional methods of road sign recognition often face challenges in handling the diverse range of signs, varying environmental conditions, and real-time processing requirements. This necessitates the exploration of AI-based solutions to overcome these limitations and enhance road sign recognition capabilities. AI-powered road sign recognition systems can effectively address the complexity and variability of road signs. The problem lies in the diverse shapes, colors, sizes, and designs of road signs, making it challenging for traditional algorithms to accurately detect and classify them. AI, specifically deep learning algorithms, can learn intricate patterns and features from large datasets, enabling the development of robust models capable of accurately identifying and classifying various types of road signs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e5194-6a32-416c-bb09-8b7f005ed86b",
   "metadata": {},
   "source": [
    "## Methods <a name=\"Methods\"></a>\n",
    "\n",
    "In the realm of road sign recognition, advancements in computer vision techniques have paved the way for effective detection and classification methods. One such approach involves the utilization of features like Hue, Histogram of Oriented Gradients (HOG), and Haar-like features, coupled with Support Vector Machine (SVM) classification and the random forest algorthm.\n",
    "\n",
    "Hue is a color attribute that represents the dominant color in an image. By extracting the hue component from road sign images, we can leverage its distinctive characteristics to differentiate between different types of signs. This feature provides valuable information for the classification process, aiding in accurate recognition.\n",
    "\n",
    "The Histogram of Oriented Gradients (HOG) technique captures the distribution of gradient orientations within an image. By analyzing the intensity gradients, HOG can effectively capture the shape and edges of road signs. These features serve as strong discriminative cues, enabling the system to identify and classify signs based on their unique shapes and contours.\n",
    "\n",
    "Additionally, Haar-like features offer an efficient approach to analyze patterns within an image. These features involve comparing adjacent rectangular regions to detect specific visual patterns, such as edges, corners, or color contrasts. By applying Haar-like features to road sign images, the system can identify characteristic patterns that are indicative of different types of signs.\n",
    "\n",
    "To classify the extracted features, Support Vector Machines (SVM) are commonly employed. SVM is a powerful machine learning algorithm that can efficiently categorize data points based on their features. By training an SVM model with labeled road sign images and their corresponding extracted features, the system can learn to accurately classify unseen road signs based on the learned patterns and feature representations.\n",
    "\n",
    "The Random Forest works by analyzing each road sign based on its features (such as shape, color, symbols), and then making a prediction to classify the sign. By combining the decisions of multiple decision trees, the Random Forest achieves higher accuracy in road sign detection and is robust to noise or other disturbances in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728c91b-9a4d-4a77-b72c-85b1bd09ac66",
   "metadata": {},
   "source": [
    "First of all we need to download the data. We'll use the GTSRB Training Images Set as our whole Dataset and divide it later to a training and test set. In order to get this Notebook working, you need to do the following steps:\n",
    "- Change the referencing folderpath to yours\n",
    "- install opencv via the terminal\n",
    "\n",
    "The Following tasks numbered in the cells (1., 2., 3., ...) are shown in the Table of Contents:\n",
    "\n",
    "1. Importing Librarys\n",
    "2. Downloading the Data\n",
    "3. Renaming the Data\n",
    "4. Extracting the HOG Features\n",
    "5. Split the Data and Train the model and evaluating - HOG -Random Forest\n",
    "6. Split the Data and Train the model and evaluating - HOG -Support Vector Machine\n",
    "7. Extracting the HUE Features\n",
    "8. Split the Data and Train the model and evaluating - HUE -Random Forest\n",
    "9. Split the Data and Train the model and evaluating - HUE -Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3875a9d8-d2dc-404b-835a-657b7ca21b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1.Importing Librarys\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imshow, imread\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import skimage.transform as transform\n",
    "import skimage.io\n",
    "import skimage.exposure\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "import cv2 # pls enter this command in your Terminal: pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "193bae88-66d0-461f-9ee7-2b334b13ec6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "# 2.Downloading the Data\n",
    "# URL of the dataset\n",
    "url = \"https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the downloaded file\n",
    "    with open(\"dataset.zip\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Extract the contents of the ZIP file\n",
    "    with zipfile.ZipFile(\"dataset.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"dataset_folder_train_images\")\n",
    "    \n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697c245b-2ab0-4125-a706-24b093f4f337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3.Renaming the Data\n",
    "# Renaming the datainfrastructure\n",
    "def rename_folders(folder_path):\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        folder_old_name = os.path.join(folder_path, folder_name)\n",
    "        folder_new_name = os.path.join(folder_path, str(int(folder_name)))\n",
    "        \n",
    "        # Rename the folder\n",
    "        os.rename(folder_old_name, folder_new_name)\n",
    "        \n",
    "# Be sure, to insert the right folderpath here!  \n",
    "rename_folders('/home/jovyan/ML Projekkkkt/App.ML/dataset_folder_train_images/GTSRB/Final_Training/Images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b09b85-178e-4d71-9812-15bec8c9f70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4.Extracting the HOG Features\n",
    "#read the dataset and put it in an array for HOG_features:\n",
    "def read_data_HOG(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for class_label in range(43):\n",
    "        class_folder = os.path.join(folder_path, str(class_label))\n",
    "        \n",
    "        for file_name in os.listdir(class_folder):\n",
    "            if file_name.endswith(\".ppm\"):\n",
    "                file_path = os.path.join(class_folder, file_name)\n",
    "                \n",
    "                # Code snippet for hogz\n",
    "                img = skimage.io.imread(file_path, plugin='matplotlib')\n",
    "                img_gray = skimage.color.rgb2gray(img)\n",
    "                reshaped_img = transform.resize(img_gray, (40, 40))\n",
    "                normalized_image = skimage.exposure.rescale_intensity(reshaped_img, in_range='image', out_range=(0, 1))\n",
    "                hog_features, hog_image = hog(normalized_image, orientations=8, pixels_per_cell=(6, 6), cells_per_block=(2, 2), visualize=True)\n",
    "                hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 5))\n",
    "                hog_features_array = hog_features.reshape(-1)\n",
    "                X.append(hog_features_array)\n",
    "                y.append(class_label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "folder_path_train = \"/home/jovyan/ML Projekkkkt/App.ML/dataset_folder_train_images/GTSRB/Final_Training/Images\"\n",
    "\n",
    "X_hog,y_hog = read_data_HOG(folder_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530fcdf5-5ccc-4a6f-90b9-0e257ac48ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest: 0.9422341239479725\n",
      "\n",
      "Class: 0\n",
      "Precision: 1.0\n",
      "Recall: 0.5526315789473685\n",
      "F1-Score: 0.711864406779661\n",
      "\n",
      "Class: 1\n",
      "Precision: 0.8961303462321792\n",
      "Recall: 0.8870967741935484\n",
      "F1-Score: 0.8915906788247213\n",
      "\n",
      "Class: 2\n",
      "Precision: 0.8329853862212944\n",
      "Recall: 0.8866666666666667\n",
      "F1-Score: 0.8589881593110872\n",
      "\n",
      "Class: 3\n",
      "Precision: 0.9375\n",
      "Recall: 0.9107142857142857\n",
      "F1-Score: 0.9239130434782609\n",
      "\n",
      "Class: 4\n",
      "Precision: 0.9140271493212669\n",
      "Recall: 0.9665071770334929\n",
      "F1-Score: 0.9395348837209303\n",
      "\n",
      "Class: 5\n",
      "Precision: 0.7864864864864864\n",
      "Recall: 0.7994505494505495\n",
      "F1-Score: 0.7929155313351498\n",
      "\n",
      "Class: 6\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Class: 7\n",
      "Precision: 0.8823529411764706\n",
      "Recall: 0.9172661870503597\n",
      "F1-Score: 0.8994708994708994\n",
      "\n",
      "Class: 8\n",
      "Precision: 0.9084249084249084\n",
      "Recall: 0.8239202657807309\n",
      "F1-Score: 0.8641114982578397\n",
      "\n",
      "Class: 9\n",
      "Precision: 0.981549815498155\n",
      "Recall: 0.9925373134328358\n",
      "F1-Score: 0.9870129870129871\n",
      "\n",
      "Class: 10\n",
      "Precision: 0.9631578947368421\n",
      "Recall: 0.9891891891891892\n",
      "F1-Score: 0.976\n",
      "\n",
      "Class: 11\n",
      "Precision: 0.8226950354609929\n",
      "Recall: 0.9830508474576272\n",
      "F1-Score: 0.8957528957528957\n",
      "\n",
      "Class: 12\n",
      "Precision: 0.9955654101995566\n",
      "Recall: 0.9977777777777778\n",
      "F1-Score: 0.9966703662597114\n",
      "\n",
      "Class: 13\n",
      "Precision: 0.9977777777777778\n",
      "Recall: 0.9933628318584071\n",
      "F1-Score: 0.9955654101995566\n",
      "\n",
      "Class: 14\n",
      "Precision: 1.0\n",
      "Recall: 0.9135802469135802\n",
      "F1-Score: 0.9548387096774192\n",
      "\n",
      "Class: 15\n",
      "Precision: 0.9917355371900827\n",
      "Recall: 1.0\n",
      "F1-Score: 0.995850622406639\n",
      "\n",
      "Class: 16\n",
      "Precision: 1.0\n",
      "Recall: 0.9888888888888889\n",
      "F1-Score: 0.9944134078212291\n",
      "\n",
      "Class: 17\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Class: 18\n",
      "Precision: 0.9778761061946902\n",
      "Recall: 0.9567099567099567\n",
      "F1-Score: 0.9671772428884026\n",
      "\n",
      "Class: 19\n",
      "Precision: 0.975609756097561\n",
      "Recall: 0.9302325581395349\n",
      "F1-Score: 0.9523809523809524\n",
      "\n",
      "Class: 20\n",
      "Precision: 0.9857142857142858\n",
      "Recall: 0.8846153846153846\n",
      "F1-Score: 0.9324324324324325\n",
      "\n",
      "Class: 21\n",
      "Precision: 1.0\n",
      "Recall: 0.9523809523809523\n",
      "F1-Score: 0.975609756097561\n",
      "\n",
      "Class: 22\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Class: 23\n",
      "Precision: 0.9907407407407407\n",
      "Recall: 0.981651376146789\n",
      "F1-Score: 0.9861751152073732\n",
      "\n",
      "Class: 24\n",
      "Precision: 1.0\n",
      "Recall: 0.8813559322033898\n",
      "F1-Score: 0.936936936936937\n",
      "\n",
      "Class: 25\n",
      "Precision: 0.9245283018867925\n",
      "Recall: 0.9865771812080537\n",
      "F1-Score: 0.9545454545454546\n",
      "\n",
      "Class: 26\n",
      "Precision: 0.9137931034482759\n",
      "Recall: 0.8688524590163934\n",
      "F1-Score: 0.8907563025210085\n",
      "\n",
      "Class: 27\n",
      "Precision: 1.0\n",
      "Recall: 0.7872340425531915\n",
      "F1-Score: 0.880952380952381\n",
      "\n",
      "Class: 28\n",
      "Precision: 0.9651162790697675\n",
      "Recall: 0.8383838383838383\n",
      "F1-Score: 0.8972972972972972\n",
      "\n",
      "Class: 29\n",
      "Precision: 1.0\n",
      "Recall: 0.8813559322033898\n",
      "F1-Score: 0.936936936936937\n",
      "\n",
      "Class: 30\n",
      "Precision: 0.9555555555555556\n",
      "Recall: 0.9052631578947369\n",
      "F1-Score: 0.9297297297297298\n",
      "\n",
      "Class: 31\n",
      "Precision: 0.9937888198757764\n",
      "Recall: 1.0\n",
      "F1-Score: 0.9968847352024921\n",
      "\n",
      "Class: 32\n",
      "Precision: 0.9761904761904762\n",
      "Recall: 1.0\n",
      "F1-Score: 0.9879518072289156\n",
      "\n",
      "Class: 33\n",
      "Precision: 0.9785714285714285\n",
      "Recall: 0.9927536231884058\n",
      "F1-Score: 0.9856115107913668\n",
      "\n",
      "Class: 34\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Class: 35\n",
      "Precision: 0.9955156950672646\n",
      "Recall: 0.9910714285714286\n",
      "F1-Score: 0.9932885906040269\n",
      "\n",
      "Class: 36\n",
      "Precision: 1.0\n",
      "Recall: 0.9875\n",
      "F1-Score: 0.9937106918238994\n",
      "\n",
      "Class: 37\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-Score: 1.0\n",
      "\n",
      "Class: 38\n",
      "Precision: 0.9975786924939467\n",
      "Recall: 0.9856459330143541\n",
      "F1-Score: 0.9915764139590855\n",
      "\n",
      "Class: 39\n",
      "Precision: 1.0\n",
      "Recall: 0.9827586206896551\n",
      "F1-Score: 0.9913043478260869\n",
      "\n",
      "Class: 40\n",
      "Precision: 1.0\n",
      "Recall: 0.9666666666666667\n",
      "F1-Score: 0.983050847457627\n",
      "\n",
      "Class: 41\n",
      "Precision: 1.0\n",
      "Recall: 0.9574468085106383\n",
      "F1-Score: 0.9782608695652174\n",
      "\n",
      "Class: 42\n",
      "Precision: 1.0\n",
      "Recall: 0.975609756097561\n",
      "F1-Score: 0.9876543209876543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.Split the Data and Train the model and evaluating - HOG - Random Forest\n",
    "#random Forest for HOG_Features\n",
    "X_train_forest_hog, X_test_forest_hog, y_train_forest_hog, y_test_forest_hog = train_test_split(X_hog, y_hog, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_forest_hog = scaler.fit_transform(X_train_forest_hog)\n",
    "X_test_scaled_forest_hog = scaler.transform(X_test_forest_hog)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "random_forest_hog = RandomForestClassifier()\n",
    "random_forest_hog.fit(X_train_scaled_forest_hog, y_train_forest_hog)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_forest_hog = random_forest_hog.predict(X_test_scaled_forest_hog)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_forest_hog = accuracy_score(y_test_forest_hog, y_pred_forest_hog)\n",
    "print(\"Accuracy for Random Forest:\", accuracy_forest_hog)\n",
    "print()\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision_forest_hog = precision_score(y_test_forest_hog, y_pred_forest_hog, average=None)\n",
    "recall_forest_hog = recall_score(y_test_forest_hog, y_pred_forest_hog, average=None)\n",
    "f1_forest_hog = f1_score(y_test_forest_hog, y_pred_forest_hog, average=None)\n",
    "\n",
    "# Printing the evaluation metrics for each class\n",
    "for class_label, prec, rec, f1_score_rf1 in zip(np.unique(y_hog), precision_forest_hog, recall_forest_hog, f1_forest_hog):\n",
    "    print(f\"Class: {class_label}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    print(f\"F1-Score: {f1_score_rf1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ea0a6d-9fa0-4a63-94b2-87201d246d74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a SVM: 0.9802346340219332\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m precision_svm_hog \u001b[38;5;241m=\u001b[39m precision_score(y_test_svm_hog, y_pred_svm_hog, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m recall_svm_hog \u001b[38;5;241m=\u001b[39m recall_score(y_test_svm_hog, y_pred_svm_hog, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m f1_svm_hog \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_svm_hog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_svm_hog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Printing the evaluation metrics for each class\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_label, prec, rec, f1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_hog), precision_svm_hog, recall_svm_hog, f1_svm_hog):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# 6.Split the Data and Train the model and evaluating - HOG - Support Vector Machine\n",
    "#SVM for HOG_Features\n",
    "X_train_svm_hog, X_test_svm_hog, y_train_svm_hog, y_test_svm_hog = train_test_split(X_hog, y_hog, test_size=0.2, random_state=42)\n",
    "\n",
    "clf_hog= make_pipeline(StandardScaler(),SVC(kernel=\"rbf\", gamma= \"auto\"))\n",
    "clf_hog.fit(X_train_svm_hog,y_train_svm_hog)\n",
    "y_pred_svm_hog = clf_hog.predict(X_test_svm_hog)\n",
    "accuracy_svm_hog = accuracy_score(y_test_svm_hog, y_pred_svm_hog)\n",
    "print(\"Accuracy for a SVM:\", accuracy_svm_hog)\n",
    "print()\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision_svm_hog = precision_score(y_test_svm_hog, y_pred_svm_hog, average=None)\n",
    "recall_svm_hog = recall_score(y_test_svm_hog, y_pred_svm_hog, average=None)\n",
    "f1_svm_hog = f1_score(y_test_svm_hog, y_pred_svm_hog, average=None)\n",
    "\n",
    "# Printing the evaluation metrics for each class\n",
    "for class_label, prec, rec, f1_score_svm1  in zip(np.unique(y_hog), precision_svm_hog, recall_svm_hog, f1_svm_hog):\n",
    "    print(f\"Class: {class_label}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    print(f\"F1-Score: {f1_score_svm1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f84a34e-1442-4572-b7a2-155d267e908e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m     38\u001b[0m folder_path_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/ML Projekkkkt/App.ML/dataset_folder_train_images/GTSRB/Final_Training/Images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m X_hue,y_hue \u001b[38;5;241m=\u001b[39m \u001b[43mread_data_HUE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m, in \u001b[0;36mread_data_HUE\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m desired_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m43\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m     class_folder \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;28mstr\u001b[39m(class_label))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(class_folder):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ppm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 7.Extracting the HUE Features\n",
    "def read_data_HUE(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    desired_width = 29\n",
    "    desired_height = 30\n",
    "\n",
    "    for class_label in range(43):\n",
    "        class_folder = os.path.join(folder_path, str(class_label))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for file_name in os.listdir(class_folder):\n",
    "            if file_name.endswith(\".ppm\"):\n",
    "                file_path = os.path.join(class_folder, file_name)\n",
    "                \n",
    "                \n",
    "            \n",
    "                image = cv2.imread(file_path)\n",
    "                scaled_image = cv2.resize(image, (desired_width, desired_height)) \n",
    "                image_hsv = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2HSV) \n",
    "                \n",
    "                border_size = 5\n",
    "                image_hsv_cropped = image_hsv[border_size:-border_size, border_size:-border_size] \n",
    "                hue_values = image_hsv_cropped[:, :, 0].flatten()\n",
    "                \n",
    "                histogram, _ = np.histogram(hue_values, bins=256, range=[0, 256])\n",
    "                normalized_histogram = histogram.astype(np.float64) / np.sum(histogram)\n",
    "                dimensionality = len(normalized_histogram)\n",
    "               \n",
    "                \n",
    "                X.append(hue_values)\n",
    "                y.append(class_label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "folder_path_train = \"/home/jovyan/ML Projekkkkt/App.ML/dataset_folder_train_images/GTSRB/Final_Training/Images\"\n",
    "\n",
    "X_hue,y_hue = read_data_HUE(folder_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f60ba53-ad18-4a9f-9347-42719475e5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 8.Split the Data and Train the model and evaluating - HUE - Random Forest\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#random Forest for HUE features\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train_forest_hue, X_test_forest_hue, y_train_forest_hue, y_test_forest_hue \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X_hue, y_hue, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      5\u001b[0m X_train_scaled_forest_hue \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train_forest_hue)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# 8.Split the Data and Train the model and evaluating - HUE - Random Forest\n",
    "#random Forest for HUE features\n",
    "X_train_forest_hue, X_test_forest_hue, y_train_forest_hue, y_test_forest_hue = train_test_split(X_hue, y_hue, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_forest_hue = scaler.fit_transform(X_train_forest_hue)\n",
    "X_test_scaled_forest_hue = scaler.transform(X_test_forest_hue)\n",
    "\n",
    "# Create and train the Random Forest model\n",
    "random_forest_hue = RandomForestClassifier()\n",
    "random_forest_hue.fit(X_train_scaled_forest_hue, y_train_forest_hue)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_forest_hue = random_forest_hue.predict(X_test_scaled_forest_hue)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_forest_hue = accuracy_score(y_test_forest_hue, y_pred_forest_hue)\n",
    "print(\"Accuracy for Random Forest:\", accuracy_forest_hue)\n",
    "print()\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision_forest_hue = precision_score(y_test_forest_hue, y_pred_forest_hue, average=None)\n",
    "recall_forest_hue = recall_score(y_test_forest_hue, y_pred_forest_hue, average=None)\n",
    "f1_forest_hue = f1_score(y_test_forest_hue, y_pred_forest_hue, average=None)\n",
    "\n",
    "# Printing the evaluation metrics for each class\n",
    "for class_label, prec, rec, f1_score_rf2 in zip(np.unique(y_hue), precision_forest_hue, recall_forest_hue, f1_forest_hue):\n",
    "    print(f\"Class: {class_label}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    print(f\"F1-Score: {f1_score_rf2}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cec566-68d2-4246-b512-7e36cdcda6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Split the Data and Train the model and evaluating - HUE - Support Vector Machine\n",
    "#SVM for HUE features\n",
    "X_train_svm_hue, X_test_svm_hue, y_train_svm_hue, y_test_svm_hue = train_test_split(X_hue, y_hue, test_size=0.2, random_state=42)\n",
    "\n",
    "clf_hue= make_pipeline(StandardScaler(),SVC(kernel=\"rbf\", gamma= \"auto\"))\n",
    "clf_hue.fit(X_train_svm_hue,y_train_svm_hue)\n",
    "y_pred_svm_hue = clf_hue.predict(X_test_svm_hue)\n",
    "accuracy_svm_hue = accuracy_score(y_test_svm_hue, y_pred_svm_hue)\n",
    "print(\"Accuracy for a SVM:\", accuracy_svm_hue)\n",
    "print()\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision_svm_hue = precision_score(y_test_svm_hue, y_pred_svm_hue, average=None)\n",
    "recall_svm_hue = recall_score(y_test_svm_hue, y_pred_svm_hue, average=None)\n",
    "f1_svm_hue = f1_score(y_test_svm_hue, y_pred_svm_hue, average=None)\n",
    "\n",
    "# Printing the evaluation metrics for each class\n",
    "for class_label, prec, rec, f1_score_svm2 in zip(np.unique(y_hue), precision_svm_hue, recall_svm_hue, f1_svm_hue):\n",
    "    print(f\"Class: {class_label}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    print(f\"F1-Score: {f1_score_svm2}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
